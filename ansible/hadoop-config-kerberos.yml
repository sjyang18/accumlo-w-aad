---
- name: deploy namenode spn and keytab
  hosts: namenode
  vars:
    hadoop_spn_owner_mapping:
      nn: hdfs
  become: no
  tasks:
    - import_tasks: tasks/common_vars.yml
    - name: set variables
      set_fact:
        ldap_spn: "{{ item.key }}"
        ldap_spn_owner: "{{ item.value }}"
        ldap_spn_w_fqdn: "{{ item.key }}/{{ ansible_fqdn }}@{{ realm_name }}"
      with_dict: "{{ hadoop_spn_owner_mapping }}"
    - import_tasks: tasks/deploy_keytab.yml

- name: deploy datanode spn and keytab
  hosts: workers
  vars:
    hadoop_spn_owner_mapping:
      dn: hdfs
  become: no
  tasks:
    - import_tasks: tasks/common_vars.yml
    - name: set variables
      set_fact:
        ldap_spn: "{{ item.key }}"
        ldap_spn_owner: "{{ item.value }}"
        ldap_spn_w_fqdn: "{{ item.key }}/{{ ansible_fqdn }}@{{ realm_name }}"
      with_dict: "{{ hadoop_spn_owner_mapping }}"
    - import_tasks: tasks/deploy_keytab.yml
    
- name: deploy rm spn and keytab
  hosts: resourcemanager
  vars:
    hadoop_spn_owner_mapping:
      rm: yarn
  become: no
  tasks:
    - import_tasks: tasks/common_vars.yml
    - name: set variables
      set_fact:
        ldap_spn: "{{ item.key }}"
        ldap_spn_owner: "{{ item.value }}"
        ldap_spn_w_fqdn: "{{ item.key }}/{{ ansible_fqdn }}@{{ realm_name }}"
      with_dict: "{{ hadoop_spn_owner_mapping }}"
    - import_tasks: tasks/deploy_keytab.yml

- name: deploy nm spn and keytab
  hosts: resourcemanager
  vars:
    hadoop_spn_owner_mapping:
      nm: yarn
  become: no
  tasks:
    - import_tasks: tasks/common_vars.yml
    - name: set variables
      set_fact:
        ldap_spn: "{{ item.key }}"
        ldap_spn_owner: "{{ item.value }}"
        ldap_spn_w_fqdn: "{{ item.key }}/{{ ansible_fqdn }}@{{ realm_name }}"
      with_dict: "{{ hadoop_spn_owner_mapping }}"
    - import_tasks: tasks/deploy_keytab.yml

- name: deploy journalnode spn and keytab
  hosts: journalnode
  vars:
    hadoop_spn_owner_mapping:
      jhs: mapred
  become: no
  tasks:
    - import_tasks: tasks/common_vars.yml
    - name: set variables
      set_fact:
        ldap_spn: "{{ item.key }}"
        ldap_spn_owner: "{{ item.value }}"
        ldap_spn_w_fqdn: "{{ item.key }}/{{ ansible_fqdn }}@{{ realm_name }}"
      with_dict: "{{ hadoop_spn_owner_mapping }}"
    - import_tasks: tasks/deploy_keytab.yml

- name: deploy HTTP spn and keytab
  hosts: hadoop
  vars:
    hadoop_spn_owner_mapping:
      HTTP: hdfs
  become: no
  tasks:
    - import_tasks: tasks/common_vars.yml
    - name: set variables
      set_fact:
        ldap_spn: "{{ item.key }}"
        ldap_spn_owner: "{{ item.value }}"
        ldap_spn_w_fqdn: "{{ item.key }}/{{ ansible_fqdn }}@{{ realm_name }}"
      with_dict: "{{ hadoop_spn_owner_mapping }}"
    - import_tasks: tasks/deploy_keytab.yml

- name: changes the existing hadoop configs with kerberos
  hosts: hadoop
  gather_facts: yes
  become: no
  tasks:
    - import_tasks: tasks/common_vars.yml
    - name: Set keberos keytab files and service principal names
      set_fact:
        nn_service_principal_keytabfile: "{{ keytabs_deployment_dir }}/nn.keytab"
        nn_service_principal: "nn/_HOST@{{ realm_name }}"
        zkfc_service_principal_w_fqdn: "zookeeper/{{ ansible_fqdn }}@{{ realm_name }}"
        zkfc_service_principal_keytabfile: "{{ keytabs_deployment_dir }}/zookeeper.keytab"
        dn_service_principal_keytabfile: "{{ keytabs_deployment_dir }}/dn.keytab"
        dn_service_principal: "dn/_HOST@{{ realm_name }}"
        rm_service_principal_keytabfile: "{{ keytabs_deployment_dir }}/rm.keytab"
        rm_service_principal_w_fqdn: "rm/{{ ansible_fqdn }}@{{ realm_name }}"
        rm_service_principal: "rm/_HOST@{{ realm_name }}"
        nm_service_principal_keytabfile: "{{ keytabs_deployment_dir }}/nm.keytab"
        nm_service_principal: "nm/_HOST@{{ realm_name }}"
        nm_service_principal_w_fqdn: "nm/{{ ansible_fqdn }}@{{ realm_name }}"
        jhs_service_principal_keytabfile: "{{ keytabs_deployment_dir }}/jhs.keytab"
        jhs_service_principal: "jhs/_HOST@{{ realm_name }}"
        http_service_principal_keytabfile: "{{ keytabs_deployment_dir }}/HTTP.keytab"
        http_service_principal: "HTTP/_HOST@{{ realm_name }}"

    - name: Generate hdfs_zkfc.jaas
      template:
        src=conf/hdfs_zkfc.jaas.j2
        dest='{{ hadoop_home }}/etc/hadoop/hdfs_zkfc.jaas'

    - name: Update HDFS_ZKFC_OPTS with Kerberos setting
      blockinfile:
        path: '{{ hadoop_home }}/etc/hadoop/hadoop-env.sh'
        state: present
        insertafter: EOF
        marker: "# {mark} ANSIBLE MANAGED BLOCK for Kerberos"
        block: |
          export CLIENT_JVMFLAGS
          export HDFS_ZKFC_OPTS="\
            -Djavax.security.auth.useSubjectCredsOnly=false \
            -Djava.security.auth.login.config={{ hadoop_home }}/etc/hadoop/hdfs_zkfc.jaas \
            -Dzookeeper.sasl.client=true \
            -Dzookeeper.sasl.client.username=zookeeper \
            -Dzookeeper.sasl.clientconfig=Client ${HDFS_ZKFC_OPTS}"

    - name: update etc/hadoop/core-site.xml with Kerberos setting
      blockinfile:
        path: '{{ hadoop_home }}/etc/hadoop/core-site.xml'
        insertbefore: '\<\/configuration\>$'
        marker: "<!-- {mark} ANSIBLE MANAGED BLOCK for Kerberos -->"
        block: |
          <property>
              <name>hadoop.security.authentication</name>
              <value>kerberos</value>
          </property>
          <property>
              <name>hadoop.security.authorization</name>
              <value>true</value>
          </property>
          <property>
                <name>hadoop.rpc.protection</name>
                <value>authentication,privacy</value>
          </property>

          <property>
                <name>hadoop.proxyuser.hdfs.groups</name>
                <value>*</value>
          </property>

          <property>
              <name>hadoop.proxyuser.hdfs.hosts</name>
              <value>*</value>
          </property>

          <property>
            <name>hadoop.proxyuser.HTTP.groups</name>
            <value>users</value>
          </property>

          <property>
            <name>hadoop.proxyuser.yarn.groups</name>
            <value>*</value>
          </property>

          <property>
            <name>hadoop.security.auth_to_local</name>
            <value>
          RULE:[1:$1@$0](.*@{{ realm_name }})s/@.*///L
          RULE:[2:$1@$0]({{ domain_admin_username }}@{{ realm_name }})s/.*/{{ domain_admin_username }}/
          DEFAULT</value>
          </property>

          <property>
                <name>hadoop.proxyuser.yarn.hosts</name>
                <value>{{ groups['resourcemanager'][0] }}.{{ domain_name }},{{ groups['resourcemanager'][1] }}.{{ domain_name }}</value>
          </property>
   
    - name: update etc/hadoop/hdfs-site.xml with Kerberos setting
      blockinfile:
        path: '{{ hadoop_home }}/etc/hadoop/hdfs-site.xml'
        insertbefore: '\<\/configuration\>$'
        marker: "<!-- {mark} ANSIBLE MANAGED BLOCK for Kerberos -->"
        block: |
          <!-- Namenode -->

          <property>
                <name>dfs.block.access.token.enable</name>
                <value>true</value>
          </property>

          <property>
                <name>dfs.namenode.kerberos.principal</name>
                <value>{{ nn_service_principal }}</value>
          </property>

          <property>
                <name>dfs.namenode.keytab.file</name>
                <value>{{ nn_service_principal_keytabfile }}</value>
          </property>

          <property>
                <name>dfs.namenode.kerberos.internal.spnego.principal</name>
                <value>{{ http_service_principal }}</value>
          </property>

          <property>
                <name>dfs.web.authentication.kerberos.keytab</name>
                <value>{{ http_service_principal_keytabfile }}</value>
          </property>
          <property>
                <name>dfs.http.policy</name>
                <value>HTTPS_ONLY</value>
          </property>

          <property>
              <name>dfs.webhdfs.enabled</name>
              <value>true</value>
          </property>

          <property>
              <name>dfs.encrypt.data.transfer</name>
              <value>true</value>
          </property>

          <!-- JournalNode -->

          <property>
                <name>dfs.journalnode.kerberos.principal</name>
                <value>{{ jhs_service_principal }}</value>
          </property>

          <property>
                <name>dfs.journalnode.keytab.file</name>
                <value>{{ jhs_service_principal_keytabfile }}</value>
          </property>

          <property>
                <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
                <value>{{ http_service_principal }}</value>
          </property>

          <property>
                <name>dfs.journalnode.https-address</name>
                <value>0.0.0.0:8481</value>
          </property>

          <!-- DataNode -->

          <property>
                <name>dfs.datanode.data.dir.perm</name>
                <value>750</value>
          </property>

          <property>
                <name>dfs.datanode.address</name>
                <value>0.0.0.0:1039</value>
                <description> This must be non-privileged port (i.e. > 1024) for
                SASL to authenticate Data Transfer Protocol (DTP) </description>
          </property>

          <property>
                <name>dfs.datanode.https.address</name>
                <value>0.0.0.0:50075</value>
          </property>

          <property>
                <name>dfs.datanode.kerberos.principal</name>
                <value>{{ dn_service_principal }}</value>
          </property>

          <property>
                <name>dfs.datanode.keytab.file</name>
                <value>{{ dn_service_principal_keytabfile }}</value>
          </property>

          <property>
              <name>dfs.encrypt.data.transfer</name>
              <value>true</value>
          </property>

          <property>
                <name>dfs.encrypt.data.transfer.cipher.suites</name>
                <value>AES/CTR/NoPadding</value>
          </property>

          <property>
                <name>dfs.data.transfer.protection</name>
                <value>authentication,privacy</value>
          </property>

          <!-- WebHDFS -->

          <property>
                <name>dfs.web.authentication.kerberos.principal</name>
                <value>{{  http_service_principal }}</value>
          </property>

          <property>
                <name>dfs.web.authentication.kerberos.keytab</name>
                <value>{{ http_service_principal_keytabfile }}</value>
          </property>

          <property>
                <name>dfs.permissions.superusergroup</name>
                <value>{{ supergroup }}</value>
          </property>

    - name: Generate yarn_jaas.conf
      template:
        src=conf/yarn_jaas.conf.j2
        dest='{{ hadoop_home }}/etc/hadoop/yarn_jaas.conf'

    - name: Generate yarn_nm_jaas.conf
      template:
        src=conf/yarn_nm_jaas.conf.j2
        dest='{{ hadoop_home }}/etc/hadoop/yarn_nm_jaas.conf'

    - name: Update YARN_RESOURCEMANAGER_OPTS with Kerberos setting
      blockinfile:
        path: '{{ hadoop_home }}/etc/hadoop/yarn-env.sh'
        state: present
        insertafter: EOF
        marker: "# {mark} ANSIBLE MANAGED BLOCK for Kerberos"
        block: |
          export YARN_RESOURCEMANAGER_OPTS="\
          -Djavax.security.auth.useSubjectCredsOnly=false \
          -Djava.security.auth.login.config={{ hadoop_home }}/etc/hadoop/yarn_jaas.conf \
          -Dzookeeper.sasl.client=true \
          -Dzookeeper.sasl.client.username=zookeeper \
          -Dzookeeper.sasl.clientconfig=Client \
          ${YARN_RESOURCEMANAGER_OPTS} ${CLIENT_JVMFLAGS}"
          export YARN_NODEMANAGER_OPTS="\
           -Djava.security.auth.login.config={{ hadoop_home }}/etc/hadoop/yarn_nm_jaas.conf \
           -Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username=zookeeper \
           -Dzookeeper.sasl.clientconfig=Client ${YARN_NODEMANAGER_OPTS} ${CLIENT_JVMFLAGS}"

    - name: update etc/hadoop/yarn-site.xml with Kerberos setting
      blockinfile:
        path: '{{ hadoop_home }}/etc/hadoop/yarn-site.xml'
        insertbefore: '\<\/configuration\>$'
        marker: "<!-- {mark} ANSIBLE MANAGED BLOCK for Kerberos -->"
        block: |
          <!-- Resource Manager -->
          <property>
            <name>yarn.resourcemanager.principal</name>
            <value>{{ rm_service_principal }}</value>
          </property>
          <property>
            <name>yarn.resourcemanager.keytab</name>
            <value>{{ rm_service_principal_keytabfile }}</value>
          </property>
          <!-- NodeManager -->
          <property>
            <name>yarn.nodemanager.principal</name>
            <value>{{ nm_service_principal }}</value>
          </property>
          <property>
            <name>yarn.nodemanager.keytab</name>
            <value>{{ nm_service_principal_keytabfile }}</value>
          </property>
          <property>
            <name>yarn.nodemanager.container-executor.class</name>
            <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
          </property>
          <property>
            <name>yarn.nodemanager.linux-container-executor.group</name>
            <value>{{ default_service_principal_login }}</value>
          </property>

    - name: update etc/hadoop/mapred-site.xml with Kerberos setting
      blockinfile:
        path: '{{ hadoop_home }}/etc/hadoop/mapred-site.xml'
        insertbefore: '\<\/configuration\>$'
        marker: "<!-- {mark} ANSIBLE MANAGED BLOCK for Kerberos -->"
        block: |
          <!-- Job History Server -->
          <property>
            <name>mapreduce.jobhistory.keytab</name>
            <value>{{ jhs_service_principal_keytabfile }}</value>
          </property>
          <property>
            <name>mapreduce.jobhistory.principal</name>
            <value>{{ jhs_service_principal }}</value>
          </property>
